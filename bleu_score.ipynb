{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bleu_score.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"qVPcWJpIaU32","colab_type":"code","colab":{}},"source":["from collections import defaultdict\n","from matplotlib import pyplot as plt\n","from nltk.translate.bleu_score import sentence_bleu\n","import torch\n","\n","def one_hot_to_text(one_hot, languageModel, filter_unk=False):\n","  return [languageModel.vocab.itos[idx] for idx in one_hot if not (filter_unk and idx == 0)]\n","\n","def experiment(model, iterator):\n","    counts = defaultdict(float)\n","    scores = defaultdict(float)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            if i != 0: break\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 1)\n","            sentence_size, batch_size, vocab_size = output.shape\n","            for batch_idx in range(batch_size):\n","                probs = F.softmax(output[:,batch_idx,:], 1)\n","                print(probs.shape)\n","                _, sentence_by_idx = probs.max(axis=1)\n","                print(sentence_by_idx)\n","                    \n","                score = sentence_bleu([one_hot_to_text(trg[:,batch_idx], targetLanguage)], one_hot_to_text(sentence_by_idx, targetLanguage))\n","                counts[len(src.view(-1))] += 1\n","                scores[len(src.view(-1))] += score\n","\n","    return {length: scores[length] / counts[length] for length in scores}\n","\n","def plot_scores(model, iterator):\n","    results = experiment(model, iterator)\n","    ks = [k for k in results]\n","    plt.plot(ks, [results[k] for k in ks])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1ZFEj5pfao3","colab_type":"code","colab":{}},"source":["def beam_search(beam_width, max_length, model, batch, sourceLanguage, targetLanguage):\n","    batch_size = batch.trg.shape[1]\n","    outputs = torch.zeros(batch_size, max_length, dtype=torch.long)\n","    encoder_output, encoder_hidden = model.encoder(batch.src)\n","\n","    for i in range(batch_size):\n","        batch_encoder_output = encoder_output[:, i, :].unsqueeze(1)\n","        beam = [(0.0, torch.LongTensor([sourceLanguage.vocab.stoi[sourceLanguage.init_token]]), encoder_hidden[i,:].unsqueeze(0))]\n","        \n","        for t in range(1, max_length):\n","            nxt = []\n","            all_terminated = 0 # to be used to detect if all viable sentences have hit eos\n","\n","            for node in beam:\n","                score, seq, hidden = node\n","\n","                # To check if this viable sentence has already hit an eos\n","                if seq[-1] == targetLanguage.vocab.stoi[targetLanguage.eos_token]:\n","                    all_terminated += 1\n","                    nxt += [node]\n","                    continue\n","\n","                decoder_output, decoder_hidden = model.decoder(torch.LongTensor([seq[-1]]).to(device), hidden, batch_encoder_output)\n","                probs = F.softmax(decoder_output.squeeze())\n","                log_probs = torch.log(probs)\n","                top_scores, top_idxs = log_probs.topk(beam_width)\n","                nxt += [(score + top_scores[b], torch.cat([seq, torch.LongTensor([top_idxs[b]])]), decoder_hidden) for b in range(beam_width)]\n","\n","            if all_terminated == beam_width: break # to check for termination\n","            beam = sorted(nxt, reverse=True, key=lambda x: x[0])[:beam_width]\n","\n","        _, output_vec, _ = max(beam, key=lambda x: x[0])\n","        sentence_length = output_vec.shape[0]\n","        outputs[i,:sentence_length] = output_vec\n","    return outputs"],"execution_count":0,"outputs":[]}]}
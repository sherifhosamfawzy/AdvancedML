# -*- coding: utf-8 -*-
"""NeuralMachineTranslation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C_Vo0iYKCbxnWVp88aSjeMB5csnFUhFl
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchtext.datasets import TranslationDataset, Multi30k
from torchtext.data import Field, BucketIterator
import spacy
import numpy as np
import random
import math
import time

SEED = 1234
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

!python -m spacy download en
!python -m spacy download fr
!python -m spacy download de

spacy_en = spacy.load('en')
spacy_fr = spacy.load('fr')
spacy_de = spacy.load('de')

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

def tokenize_de(text):
    return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_fr(text):
    return [tok.text for tok in spacy_fr.tokenizer(text)]

    
sourceLanguage = Field(tokenize = tokenize_en, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

targetLanguage = Field(tokenize = tokenize_fr, 
            init_token = '<sos>', 
            eos_token = '<eos>', 
            lower = True)

train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (sourceLanguage, targetLanguage))
sourceLanguage.build_vocab(train_data, min_freq = 2)
targetLanguage.build_vocab(train_data, min_freq = 2)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

BATCH_SIZE = 1
INPUT_DIM  = len(sourceLanguage.vocab) # Kx
OUTPUT_DIM = len(targetLanguage.vocab) # Ky
EMB_DIM = 256 # m 
HID_DIM = 512 # n
MAXOUT_DIM = 400 # l 
ATT_HID_DIM = 1000 # n'

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
    (train_data, valid_data, test_data), 
    batch_size = BATCH_SIZE,
    device = device)

class Encoder(nn.Module):
    def __init__(self, input_dimension, embedding_dimension, hidden_dimension):
        super().__init__() 
        self.embedding = nn.Embedding(input_dimension, embedding_dimension)
        self.gru = nn.GRU(embedding_dimension, hidden_dimension, bidirectional = True)
        
    def forward(self, source):  
        embedded = self.embedding(source)
        outputs, hidden = self.gru(embedded)
        hidden = torch.tanh(hidden[1,:,:])
        return outputs, hidden

class Attention(nn.Module):
    def __init__(self, hidden_dimension, attention_hidden_dimension):
        super().__init__()
        self.attn = nn.Linear((hidden_dimension * 2) + hidden_dimension, attention_hidden_dimension)
        self.v = nn.Linear(attention_hidden_dimension, 1, bias = False)
        
    def forward(self, hidden, encoder_outputs):
      
        batch_size = encoder_outputs.shape[1]
        source_length = encoder_outputs.shape[0]
        hidden = hidden.unsqueeze(1).repeat(1, source_length, 1)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)

        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        attention = self.v(energy).squeeze(2)
        return F.softmax(attention, dim=1)

class Decoder(nn.Module):
    def __init__(self, output_dimension, embedding_dimension, hidden_dimension, attention, maxout_dimension):
        super().__init__()
        self.output_dimension = output_dimension
        self.maxout_dimension = maxout_dimension
        self.attention = attention
        self.embedding = nn.Embedding(output_dimension, embedding_dimension)
        self.gru = nn.GRU((hidden_dimension * 2) + embedding_dimension , hidden_dimension)
        self.maxout = nn.Linear((hidden_dimension * 2) + hidden_dimension + embedding_dimension, 2 * maxout_dimension)
        self.fc_out = nn.Linear(maxout_dimension, output_dimension)
        
    def forward(self, input, hidden, encoder_outputs):        
        
        input = input.unsqueeze(0)        
        embedded = self.embedding(input)
        a = self.attention(hidden, encoder_outputs)
        a = a.unsqueeze(1)
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        context = torch.bmm(a, encoder_outputs)
        context = context.permute(1, 0, 2) 
        gru_input = torch.cat((embedded, context), dim = 2)

        output, hidden = self.gru(gru_input, hidden.unsqueeze(0))

        assert (output == hidden).all()
        # Since output == hidden, use output. Can't use hidden because of a shaping issue
        # Essentially, they are the same tensor but one is [1,1,x] and the other is [1,x]

        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        context = context.squeeze(0)

        t_init = self.maxout(torch.cat((output, context, embedded), dim = 1)) # Size 2xl
        t_init = t_init.view(self.maxout_dimension, 2)
        t, _ = torch.max(t_init,1)                       
        t = t.view(1,t.shape[0])  # Size l
        prediction = self.fc_out(t)

        return prediction, hidden.squeeze(0)

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def forward(self, src, trg):
        batch_size = src.shape[1]
        target_length = trg.shape[0]
        target_vocab_size = self.decoder.output_dimension
        outputs = torch.zeros(target_length, batch_size, target_vocab_size).to(self.device)
        encoder_outputs, hidden = self.encoder(src)
        input = trg[0,:] # SOS

        for t in range(1, target_length):
            output, hidden = self.decoder(input, hidden, encoder_outputs)
            outputs[t] = output
            input = output.argmax(1)
        return outputs

enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM)
attn = Attention(HID_DIM,ATT_HID_DIM)
dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, attn, MAXOUT_DIM)
model = Seq2Seq(enc, dec, device).to(device)

def init_weights(m):
    for name, param in m.named_parameters():
        print(name)
        if 'gru.weight_hh' in name:
            nn.init.orthogonal_(param.data)
        elif 'attn.weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.001)
        elif 'v.weight' in name or 'bias' in name:
            nn.init.constant_(param.data, 0)
        else:
            nn.init.normal_(param.data, mean=0, std=0.01)
            
model.apply(init_weights)

optimizer = optim.Adadelta(model.parameters(), rho=0.95, eps=1e-06)
TRG_PAD_IDX = targetLanguage.vocab.stoi[targetLanguage.pad_token]
criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)

def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        if i == 0 :

          src = batch.src
          trg = batch.trg
          optimizer.zero_grad()

          output = model(src, trg)
          output_dim = output.shape[-1]
          output = output[1:].view(-1, output_dim)
          trg = trg[1:].view(-1)

          loss = criterion(output, trg)         
          loss.backward()
          torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
          optimizer.step()
          epoch_loss += loss.item()
        
          return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):
            src = batch.src
            trg = batch.trg

            output = model(src, trg)

            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            
            loss = criterion(output, trg)
            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

N_EPOCHS = 1
CLIP = 1

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = 0 
    #valid_loss = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()
    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut3-model.pt')
    
    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')